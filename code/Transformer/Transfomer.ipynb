{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1M4kag3rQyDM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n"
      ],
      "metadata": {
        "id": "ab4UX8Kbnr_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TokenEmbedding"
      ],
      "metadata": {
        "id": "YtL6rTU6f2Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "# 将输入词汇表的索引转化为指定维度的Embedding\n",
        "\n",
        "class TokenEmbedding(nn.Embedding):\n",
        "  def __init__(self,vocab_size,d_model):\n",
        "    super(TokenEmbedding,self).__init__(vocab_size,d_model,padding_idx=1)"
      ],
      "metadata": {
        "id": "_oHqX07ieKD7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Positional Embedding"
      ],
      "metadata": {
        "id": "4WJRgFNFf8wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,d_model,max_len):\n",
        "    super(PositionalEmbedding,self).__init__()\n",
        "    self.encoding=torch.zeros(max_len,d_model)\n",
        "    self.encoding.requires_grad=False\n",
        "    pos=torch.arange(0,max_len)\n",
        "    pos=pos.float().unsqueeze(dim=1)\n",
        "    _2i=torch.arange(0,d_model,step=2).float()\n",
        "    self.encoding[:,0::2]=torch.sin(pos/10000**(_2i/d_model))\n",
        "    self.encoding[:,1::2]=torch.cos(pos/10000**(_2i/d_model))\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_len=x.size()\n",
        "    return self.encoding[:seq_len,:]"
      ],
      "metadata": {
        "id": "JREj6pFzf0EZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TransformerEmbedding"
      ],
      "metadata": {
        "id": "rRYu3-yml56m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,max_len,drop_prob):\n",
        "    super(TransformerEmbedding,self).__init__()\n",
        "    self.tok_emb=TokenEmbedding(vocab_size,d_model)\n",
        "    self.pos_emb=PositionalEmbedding(d_model,max_len)\n",
        "    self.drop_prob=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    tok_emb=self.tok_emb(x)\n",
        "    pos_emb=self.pos_emb(x)\n",
        "    return self.drop_prob(tok_emb+pos_emb)"
      ],
      "metadata": {
        "id": "Jlzfe823l6kl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "nIYLBFOGqoEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.rand(128,32,512)"
      ],
      "metadata": {
        "id": "vTFtMDPjtfuw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,n_head):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    self.n_head=n_head\n",
        "    self.d_model=d_model\n",
        "    self.w_q=nn.Linear(d_model,d_model)\n",
        "    self.w_k=nn.Linear(d_model,d_model)\n",
        "    self.w_v=nn.Linear(d_model,d_model)\n",
        "    self.w_combine=nn.Linear(d_model,d_model)\n",
        "    self.softmax=nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    batch,_,_=q.shape\n",
        "    _,time_q,_=q.shape\n",
        "    _,time_k,_=k.shape\n",
        "    _,time_v,_=v.shape\n",
        "    n_d=self.d_model//self.n_head\n",
        "    q,k,v=self.w_q(q),self.w_k(k),self.w_v(v)\n",
        "    q=q.view(batch,time_q,self.n_head,n_d).permute(0,2,1,3)\n",
        "    k=k.view(batch,time_k,self.n_head,n_d).permute(0,2,1,3)\n",
        "    v=v.view(batch,time_v,self.n_head,n_d).permute(0,2,1,3)\n",
        "    score=q@k.transpose(-2,-1)/math.sqrt(n_d)\n",
        "    if mask is not None:\n",
        "      score=score.masked_fill(mask==0,float('-inf'))\n",
        "    score=self.softmax(score)@v\n",
        "    score=score.permute(0,2,1,3).contiguous().view(batch,time_q,self.d_model)\n",
        "    out=self.w_combine(score)\n",
        "    return out"
      ],
      "metadata": {
        "id": "omXDdnuUq5aG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LayerNorm"
      ],
      "metadata": {
        "id": "cD7fEw7gb8Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,d_model,eps=1e-12):\n",
        "    super(LayerNorm,self).__init__()\n",
        "    self.gamma=nn.Parameter(torch.ones(d_model))\n",
        "    self.beta=nn.Parameter(torch.zeros(d_model))\n",
        "    self.eps=eps\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(-1,keepdim=True)\n",
        "    var=x.var(-1,unbiased=False,keepdim=True)\n",
        "    out=(x-mean)/torch.sqrt(var+self.eps)\n",
        "    out=self.gamma*out+self.beta\n",
        "    return out"
      ],
      "metadata": {
        "id": "VRdeCqzLb-lv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "1rOyIOOk2qse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PositionwiseFeedForward"
      ],
      "metadata": {
        "id": "YhqyHgD-5l83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,hidden,dropout=0.1):\n",
        "    super(PositionwiseFeedForward,self).__init__()\n",
        "    self.fc1=nn.Linear(d_model,hidden)\n",
        "    self.fc2=nn.Linear(hidden,d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=F.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x=self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "uNcB-tjE2tpq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EncoderLayer"
      ],
      "metadata": {
        "id": "z3H-nvsYYZV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,n_head,dropout=0.1):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.attention=MultiHeadAttention(d_model,n_head)\n",
        "    self.dropout1=nn.Dropout(dropout)\n",
        "    self.norm1=LayerNorm(d_model)\n",
        "    self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,dropout)\n",
        "    self.dropout2=nn.Dropout(dropout)\n",
        "    self.norm2=LayerNorm(d_model)\n",
        "\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    _x=x\n",
        "    x=self.attention(x,x,x,mask)\n",
        "    x=self.dropout1(x)\n",
        "    x=self.norm1(x+_x)\n",
        "    _x=x\n",
        "    x=self.ffn(x)\n",
        "    x=self.dropout2(x)\n",
        "    x=self.norm2(x+_x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "2cow4qkvYZ0f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder"
      ],
      "metadata": {
        "id": "oYKWia3Gb29B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,enc_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,dropout=0.1):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.embedding=TransformerEmbedding(enc_voc_size,d_model,max_len,dropout)\n",
        "    self.layers=nn.ModuleList(\n",
        "      [\n",
        "        EncoderLayer(d_model,ffn_hidden,n_head,dropout)\n",
        "        for _ in range(n_layer)\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def forward(self,x,src_mask):\n",
        "    x=self.embedding(x)\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,src_mask)\n",
        "    return x"
      ],
      "metadata": {
        "id": "o_0ifcV6b00J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decoder"
      ],
      "metadata": {
        "id": "gVRiS1zMjxwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DecoderLayer"
      ],
      "metadata": {
        "id": "zHnMg9lYj4b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,n_head,dropout=0.1):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.self_attention=MultiHeadAttention(d_model,n_head)\n",
        "    self.dropout1=nn.Dropout(dropout)\n",
        "    self.norm1=LayerNorm(d_model)\n",
        "    self.cross_attention=MultiHeadAttention(d_model,n_head)\n",
        "    self.dropout2=nn.Dropout(dropout)\n",
        "    self.norm2=LayerNorm(d_model)\n",
        "    self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,dropout)\n",
        "    self.dropout3=nn.Dropout(dropout)\n",
        "    self.norm3=LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x,memory,tar_mask,src_mask):\n",
        "    _x=x\n",
        "    x=self.self_attention(x,x,x,tar_mask)\n",
        "    x=self.dropout1(x)\n",
        "    x=self.norm1(x+_x)\n",
        "    _x=x\n",
        "    x=self.cross_attention(x,memory,memory,src_mask)\n",
        "    x=self.dropout2(x)\n",
        "    x=self.norm2(x+_x)\n",
        "    _x=x\n",
        "    x=self.ffn(x)\n",
        "    x=self.dropout3(x)\n",
        "    x=self.norm3(x+_x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4q83OyuDjxWa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decoder"
      ],
      "metadata": {
        "id": "DmsruNs1A8II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,dropout=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embedding=TransformerEmbedding(dec_voc_size,d_model,max_len,dropout)\n",
        "    self.layers=nn.ModuleList(\n",
        "        [\n",
        "            DecoderLayer(d_model,ffn_hidden,n_head,dropout)\n",
        "            for _ in range(n_layer)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def forward(self,x,memory,tar_mask,src_mask):\n",
        "    x=self.embedding(x)\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,memory,tar_mask,src_mask)\n",
        "    return x"
      ],
      "metadata": {
        "id": "T_y7aTR1A9aA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "ALDaXkchGk6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "62BnQ0eRH1ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self,d_model,dec_voc_size):\n",
        "    super(Generator,self).__init__()\n",
        "    self.fc=nn.Linear(d_model,dec_voc_size)\n",
        "  def forward(self,x):\n",
        "    return self.fc(x)"
      ],
      "metadata": {
        "id": "wPGYQ6g0H09c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "hs5bK3mkH4FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,enc_voc_size,dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,dropout=0.1):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.encoder=Encoder(enc_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,dropout)\n",
        "    self.decoder=Decoder(dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,dropout)\n",
        "    self.generator=Generator(d_model,dec_voc_size)\n",
        "\n",
        "  def forward(self,src,tar,src_mask,tar_mask):\n",
        "    memory=self.encoder(src,src_mask)\n",
        "    out=self.decoder(tar,memory,tar_mask,src_mask)\n",
        "    out=self.generator(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "mfH4W9y8Gkhx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "Dzm4apgMSeVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 假设参数\n",
        "batch = 2\n",
        "src_len = 5\n",
        "tgt_len = 6\n",
        "src_vocab = 20\n",
        "tgt_vocab = 30\n",
        "d_model = 16\n",
        "\n",
        "# 随机输入\n",
        "src = torch.randint(0, src_vocab, (batch, src_len))\n",
        "tar = torch.randint(0, tgt_vocab, (batch, tgt_len))\n",
        "\n",
        "# 简单 mask (全1，表示不屏蔽任何位置)\n",
        "src_mask = torch.ones((batch, 1, 1, src_len), dtype=torch.bool)\n",
        "tar_mask = torch.ones((batch, 1, tgt_len, tgt_len), dtype=torch.bool)\n",
        "\n",
        "# 初始化 Transformer\n",
        "model = Transformer(src_vocab, tgt_vocab, max_len=10, d_model=d_model,\n",
        "                    ffn_hidden=32, n_head=2, n_layer=2)\n",
        "\n",
        "# 前向传播\n",
        "out = model(src, tar, src_mask, tar_mask)\n",
        "\n",
        "print(\"输出 shape:\", out.shape)   # 预期: [batch, tgt_len, tgt_vocab]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u-HGjWnShhn",
        "outputId": "36736169-b27e-49be-ca55-2ea0e950edf8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "输出 shape: torch.Size([2, 6, 30])\n"
          ]
        }
      ]
    }
  ]
}